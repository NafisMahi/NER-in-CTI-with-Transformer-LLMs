{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqaT-_Xx1q9o",
        "outputId": "db8b8f7d-1f61-4e6e-a8d6-be7b2a32e0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MiCeKzpsxvR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tokenizers seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTZ4Vw8503gq"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "train_dataset = datasets.load_dataset('csv', data_files='/content/drive/MyDrive/ResearchTrial/sentence_labels_aptner.csv')\n",
        "valid_dataset = datasets.load_dataset('csv', data_files='/content/drive/MyDrive/ResearchTrial/sentence_labels_valid_apt.csv')\n",
        "test_dataset = datasets.load_dataset('csv', data_files='/content/drive/MyDrive/research/sentence_labels_test_apt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTJ_8-nID9qH"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69DHxOfhOdoK"
      },
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizerFast.from_pretrained(\"CyberPeace-Institute/SecureBERT-NER\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GI3EOsUrIZm"
      },
      "source": [
        "Align Label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CapmSUE4vp-N"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def convert_string_to_list(example):\n",
        "    if isinstance(example[\"tokens\"], str):\n",
        "        example[\"tokens\"] = ast.literal_eval(example[\"tokens\"])\n",
        "    if isinstance(example[\"ner_tags\"], str):\n",
        "        example[\"ner_tags\"] = ast.literal_eval(example[\"ner_tags\"])\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88W1tAM_0Sen"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(convert_string_to_list )\n",
        "\n",
        "valid_dataset = valid_dataset.map(convert_string_to_list )\n",
        "\n",
        "test_dataset = test_dataset.map(convert_string_to_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOxhTNQsP9am"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "    \"\"\"\n",
        "    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n",
        "    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n",
        "\n",
        "    Parameters:\n",
        "    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n",
        "                     - \"tokens\": list of words in a sentence.\n",
        "                     - \"ner_tags\": list of corresponding entity tags for each word.\n",
        "\n",
        "    label_all_tokens (bool): A flag to indicate whether all tokens should have labels.\n",
        "                             If False, only the first token of a word will have a label,\n",
        "                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n",
        "\n",
        "    Returns:\n",
        "    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n",
        "    \"\"\"\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # word_ids() => Return a list mapping the tokens\n",
        "        # to their actual word in the initial sentence.\n",
        "        # It Returns a list indicating the word corresponding to each token.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        # Special tokens like `<s>` and `<\\s>` are originally mapped to None\n",
        "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # set –100 as the label for these special tokens\n",
        "                label_ids.append(-100)\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # if current word_idx is != prev then its the most regular case\n",
        "                # and add the corresponding token\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # to take care of sub-words which have the same word_idx\n",
        "                # set -100 as well for them, but only if label_all_tokens == False\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "                # mask the subword representations after the first subword\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab4WYR_lrL8Y"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_valid_datasets = valid_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-ics9OJsK1t"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=85, ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcq8gktBoStb"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnVgWXqJobwr"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow0fSYEmtBA0"
      },
      "outputs": [],
      "source": [
        "import accelerate\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "args = TrainingArguments(\n",
        "\"test-ner\",\n",
        " evaluation_strategy=\"epoch\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=8,\n",
        "per_device_eval_batch_size=8,\n",
        "num_train_epochs=1,\n",
        "weight_decay=0.01,\n",
        "logging_steps=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQtoWMg-tGa3"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9cVkIuztIIw"
      },
      "outputs": [],
      "source": [
        "metric = datasets.load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJxnpZdhp4Wx"
      },
      "outputs": [],
      "source": [
        "example = train_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNYhXhLoT58a"
      },
      "outputs": [],
      "source": [
        "## FOR DNRTI dataset\n",
        "\n",
        "# def compute_metrics(eval_preds):\n",
        "#     \"\"\"\n",
        "#     Function to compute the evaluation metrics for Named Entity Recognition (NER) tasks.\n",
        "#     The function computes precision, recall, F1 score and accuracy.\n",
        "\n",
        "#     Parameters:\n",
        "#     eval_preds (tuple): A tuple containing the predicted logits and the true labels.\n",
        "\n",
        "#     Returns:\n",
        "#     A dictionary containing the precision, recall, F1 score and accuracy.\n",
        "#     \"\"\"\n",
        "\n",
        "#     label_list = [\n",
        "#     'B-HackOrg', 'I-HackOrg',  # Assuming these start from 1\n",
        "#     'B-OffAct', 'I-OffAct',\n",
        "#     'B-SamFile', 'I-SamFile',\n",
        "#     'B-SecTeam', 'I-SecTeam',\n",
        "#     'B-Tool', 'I-Tool',\n",
        "#     'B-Time', 'I-Time',\n",
        "#     'B-Purp', 'I-Purp',\n",
        "#     'B-Area', 'I-Area',\n",
        "#     'B-Idus', 'I-Idus',\n",
        "#     'B-Org', 'I-Org',\n",
        "#     'B-Way', 'I-Way',\n",
        "#     'B-Exp', 'I-Exp',\n",
        "#     'B-Features', 'I-Features','O'\n",
        "#     # Make sure you have 27 labels here including 'O'\n",
        "# ]\n",
        "#     # np.savetxt('eval_preds.txt', eval_preds, delimiter=',')\n",
        "\n",
        "#     # print(eval_preds)\n",
        "\n",
        "#     pred_logits, labels = eval_preds\n",
        "\n",
        "#     pred_logits = np.argmax(pred_logits, axis=2)\n",
        "#     # the logits and the probabilities are in the same order,\n",
        "#     # so we don’t need to apply the softmax\n",
        "\n",
        "#     # We remove all the values where the label is -100\n",
        "#     predictions = [\n",
        "#         [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "#         for prediction, label in zip(pred_logits, labels)\n",
        "#     ]\n",
        "\n",
        "#     true_labels = [\n",
        "#       [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "#        for prediction, label in zip(pred_logits, labels)\n",
        "#    ]\n",
        "#     results = metric.compute(predictions=predictions, references=true_labels)\n",
        "#     return {\n",
        "#    \"precision\": results[\"overall_precision\"],\n",
        "#    \"recall\": results[\"overall_recall\"],\n",
        "#    \"f1\": results[\"overall_f1\"],\n",
        "#   \"accuracy\": results[\"overall_accuracy\"],\n",
        "#   }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3d59FMTqtB9"
      },
      "outputs": [],
      "source": [
        "## For APTNER dataset\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "\n",
        "\n",
        "    label_list = [\n",
        "    'B-APT', 'I-APT', 'E-APT', 'S-APT',\n",
        "    'B-SECTEAM', 'I-SECTEAM', 'E-SECTEAM', 'S-SECTEAM',\n",
        "    'B-IDTY', 'I-IDTY', 'E-IDTY', 'S-IDTY',\n",
        "    'B-OS', 'I-OS', 'E-OS', 'S-OS',\n",
        "    'B-EMAIL', 'I-EMAIL', 'E-EMAIL', 'S-EMAIL',\n",
        "    'B-LOC', 'I-LOC', 'E-LOC', 'S-LOC',\n",
        "    'B-TIME', 'I-TIME', 'E-TIME', 'S-TIME',\n",
        "    'B-IP', 'I-IP', 'E-IP', 'S-IP',\n",
        "    'B-DOM', 'I-DOM', 'E-DOM', 'S-DOM',\n",
        "    'B-URL', 'I-URL', 'E-URL', 'S-URL',\n",
        "    'B-PROT', 'I-PROT', 'E-PROT', 'S-PROT',\n",
        "    'B-FILE', 'I-FILE', 'E-FILE', 'S-FILE',\n",
        "    'B-TOOL', 'I-TOOL', 'E-TOOL', 'S-TOOL',\n",
        "    'B-MD5', 'I-MD5', 'E-MD5', 'S-MD5',\n",
        "    'B-SHA1', 'I-SHA1', 'E-SHA1', 'S-SHA1',\n",
        "    'B-SHA2', 'I-SHA2', 'E-SHA2', 'S-SHA2',\n",
        "    'B-MAL', 'I-MAL', 'E-MAL', 'S-MAL',\n",
        "    'B-ENCR', 'I-ENCR', 'E-ENCR', 'S-ENCR',\n",
        "    'B-VULNAME', 'I-VULNAME', 'E-VULNAME', 'S-VULNAME',\n",
        "    'B-VULID', 'I-VULID', 'E-VULID', 'S-VULID',\n",
        "    'B-ACT', 'I-ACT', 'E-ACT', 'S-ACT', 'O'\n",
        "    ]\n",
        "\n",
        "\n",
        "    pred_logits, labels = eval_preds\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "\n",
        "    # We remove all the values where the label is -100\n",
        "    predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "    return {\n",
        "      \"precision\": results[\"overall_precision\"],\n",
        "      \"recall\": results[\"overall_recall\"],\n",
        "      \"f1\": results[\"overall_f1\"],\n",
        "      \"accuracy\": results[\"overall_accuracy\"],\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O06MxUNhVK6G"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/ResearchTrial/SECOND_ATTEMPTS/bertcasedapt/ner_model_1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3HOQ5Ukqw6i"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "   train_dataset=tokenized_datasets[\"train\"],\n",
        "   eval_dataset=tokenized_valid_datasets[\"train\"],\n",
        "   data_collator=data_collator,\n",
        "   tokenizer=tokenizer,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK1a8ZAgrRPi"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6rjlZ2GqQGf"
      },
      "outputs": [],
      "source": [
        "label_names = [\n",
        "    'B-HackOrg', 'I-HackOrg',  # Assuming these start from 1\n",
        "    'B-OffAct', 'I-OffAct',\n",
        "    'B-SamFile', 'I-SamFile',\n",
        "    'B-SecTeam', 'I-SecTeam',\n",
        "    'B-Tool', 'I-Tool',\n",
        "    'B-Time', 'I-Time',\n",
        "    'B-Purp', 'I-Purp',\n",
        "    'B-Area', 'I-Area',\n",
        "    'B-Idus', 'I-Idus',\n",
        "    'B-Org', 'I-Org',\n",
        "    'B-Way', 'I-Way',\n",
        "    'B-Exp', 'I-Exp',\n",
        "    'B-Features', 'I-Features','O'\n",
        "    # Make sure you have 27 labels here including 'O'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY4HbZJIpMd9"
      },
      "outputs": [],
      "source": [
        "### Metrics for test set\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenized_test_datasets[\"train\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEjHpyh9J8pP"
      },
      "outputs": [],
      "source": [
        "### Metrics for train set\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenized_datasets[\"train\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQxFLgw5VAJN"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained(\"/content/drive/MyDrive/ResearchTrial/BertBaseNERApt/ner_model_5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}